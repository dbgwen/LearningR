## Week 1 ## 

  ** Overview **
    General steps: raw data -> processinf script -> tidy data -> data analysis -> data communication
    Raw data: original source, hard to use for data analysis, data analysis includes processing, 
              keep record on what you did to the raw data
    Processed data: data is ready for analysis, processing includes mergng, subsetting, transforming
              standards for processing, record all steps
 
 ** 4 things you need to have **
  - Raw data
  - Tidy dataset
  - Codebook describing variable and values - units, summary of choices made, experimental design used
    Include a section for: study design, codebook, and instruction list  w/input for raw data, outout is processed data
  - Explicit and exact recipe you used to go from raw data to processed data
    
** Downloading data **
  - getwd() and setwd()
  - Be aware of relative vs absolute paths
  - file.exists() **checks to see if directory exists
        if (!file.exists("data")) {
          dir.create("data"
          ]
  - dir.create() **will create a directory if it doesn't exist
  - download.file() **downloads from internet - if you do this by hand, helps with reproducibility 
    url, destfile, and method need to specified
  - dateDownloaded <- date()

** Reading Local Files **
  - read.table("data", sep = ",", header = TRUE) **not the best for large datasets
        quote ="" (no quotes in file) - common problem in files being read
        na.strings - represents missing value
        nrows - how many rows to read of the file
        skip - number of lines to skip before reading
  - read.csv("data", sep="", header = TRUE)
  - read.csv2()

** Reading Excel Files **
  - Excel sheets most common format to share data
  - Download as .xlsx, can use xlsx package
  - read.xlsx()
  - read.xlsx2()
  - colIndex, rowIndex
  
** Reading XML **
  - Extensible markup language (XML)
  - Used to store structured data, widely used in intervnet applications
  - Extracting XML is the basis for most web scraping
  - Markup labels that give the text structure
  - Content actual text
  - tags are general labels
        <section> </section>
        <line-break /> **empty tags
        Elements are examples of tags - <Greeting> Hello, World </Greeting>
        Attributes are components of the label
       <img src ="picture.jpg" alt="picture"/>
   - rootNode[[1]] **directly access parts of XML
   - xmlSApply(rootNOde, xmlvalue)
   - /node - top level node
   - //node - node at any level
   - xpathSApply(rootNode, "//name", xmlvalue)
   - Can scrape webpages for data

** Reading JSON FIles **
   - JavaScript Object Notation (JSON)
   - Lightweight data storage
   - Common format for APIs
   - Similar structure to XML, but different differentsyntax/format
   - Data stored as
        Numbers (double)
        Strings (double quoted)
        Boolean (true or false)
        Array (ordered, comma separated enclosed in square brackets [])
        Object (unordered, comma separated collection of key:value pairs in curly brackets{})
   - library(jsonlite)
     jsonData <- fromJSON("website")
     names(jsonData$column)
   - Turn a dataset into a JSON dataset ** nice for exporting data to be used by an API
       myjson <- toJSON(dataset, pretty = TRUE)
       cat(myjson)
       dataset <- fromJSON(myjson)
       head(dataset)

** The data.table package **
  - all functions that use data.frame can use data.table
  - faster at subsetting, grouping, and updating variables, but uses slightly different syntax
  - library(data.table)
  - tables() - name of data table, rows, megabytes, columns, and key
  - Subset datatable[row, column]
  - Looking at certain values datatable[datatable$variable = "", ]
  - Subsets based on rows c(row, row)
  - subset columns using expressions
  - add new columns datatable[, w:=z^2]
  - datatable[, m:= [tmp <- (x+2); log2(tmp+5)}]
  
## Week 2 ##

** R MySQL **

  uscsDb <- dbConnect(MySQL(), user="genome", db="hg19", 
                    host="genome-mysql.cse.ucsc.edu")
  allTable<- dbListTables(hg19)
  length(allTables)
  dbListFields(hg19, "affyU133Plus2")
  
  ** Total count
   dbGetQuery(hg19, "select count(*) from affyU133plus2")
   affyData <- dbReadTable(hg19, "affyU133Plus2")
   head(affydata)
  ** Subset
    query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
    affyMis <- fetch(query); quantile(affyMiss$missMatches)

  ** Grab only a few obs
    affyMisSmall <- fetch(query, n=10); dbClearResult(query);

   ** Close the connection
    dbDisconnect(hg19)
 
    
 ** Reading Data from HDF5 **
    Used for storing large datasets. Hierarchial data format
   
    ** R HDF5 Groups
       created = h5createFroup("example.h5", "foo")
       A = matrix(1:10, nr=5, nc=2)
       h5write(A, "example.h5", "foo/A")
       B = array(seq(0,1,1,0, by=0.1, dim = c(5,2,2))
       
    ** Write dataset
       df = data.frame(1L:5L, seq(0,1,length.out=5), 
          c("ab", "cde", "fghi", "a", "s"), stringAsFactors =FALSe)
          h5write(df, "example.h5", "df")
          h5ls("example.h5")
        
    ** Web Scrapping
       Programmatically extracting data from HTML code of websites
         - Many websites might have info you want to programatically read
         - Some websites won't let you scrape their website
         - Can use the XML package to parse the HTML
             library(XML)
             url <- "website"
             html <- htmlTreeParse(url, useInternalNodes=T)
             xpathSApply(html, "//title", xmlValue)
             xpathSApply(html, "//info", xmlValue)
         - Get command in the httr package
              library(httr); html2=GET(url)
              content2 = content(html(html2, as="text")
              parsedHtml = htmlParse(content2, asText=TRUe)
              xpathSApply(parsedHtml, "//title", xmlValue)
         - Use handles
             google = handle("http://google.com")
             pg1 = GET(handle=google,path="/")
             pg2 = GET(handle=google,path="search")
           
   ** Reading from APIs
      Application Programming Interfaces
        - Can use get requests with URLs
        - httr package to get data from website
        - Need to be a member of the dev team for that website - e.g. Twitter Dev 
                  ** Example for request for home timeline
                  myapp = oautho_app("twitter", 
                                       key"youconsumerkey", secret="yousecrethere")
                  sig = sign_oauth1.0(myapp, 
                                       token="yourtokenhere", 
                                       token_secret = "yoursecrettokenhere")
                  homeTL = GET("website") **use resource URL
                  json1 = content(homeTL)
                  json2 = jsonlite::fromJSON(toJSON(json1))
                  json2[1, 1:4]
        - httr allows: GET, POST, PUT, DELETE requests if you are authorized
        - httr works well with Facebook, Google, Twitter, Github
   
   ** Reading from Other Sources
      Useful packages
        - foreign package: loads data from Minitab, S, SAS, SPSS, Stat, Systat
          read.dta (stata)
          read.mtp (Minitab)
          read.spss (SPSS)
          read.xport (SAS)
        - RODBC interfaces to multiple databases - MySQL, Microsoft Access, SQLite
        - RMongo - rmongodb
        - tuneR - read in music data
        - seewave - analyze music data
        
      Work directly with files
        - file - function to access connection to a file 
        - url
        - gzfile and bzfile - zipped filed
        - jpeg: work with images
        - readbitmap
        - png
        - EBImage in Bioconductor


## Week 3 ##
       
  ** Subsetting and Sorting
     Directly subset columns or rows
       - X[,1]
       - X[1:2, "var2"]
       - X[,"var1"]
     
     Subset using logical statements
       - X[(X$var1 <= 3 & X$var3 >11), ]
       - X[(X$var1 <= 3 | X$var3 > 15), ]
     
     Missing values
       - X[which(X$var2 > 8), ] **won't return the NAs
     
     Sort Variables
       - sort(X$var1)
       - sort(X$var1,decreasing=TRUE)
       - sort(X$var2, na.last=TRUE)
       
     Ordering dataframe by variable
       - X[order(X$var1), ]
       - X[order(X$var1, X$var3), ]
       - library(plyr)
         arrange(X, var1)
         arrange(X, desc(var1))
         
     Adding Rows and Columns
       - X$var4 <- rnorm(5)
       - Y <- cbind(X, rnorm(5)) **column bind
       - Y <- rbind(X, rnomr(5)) **row bind
     
  ** Summarizing Data
     Example of summarizing data
      
      Getting data from web
       - if(!file.exists("./data")){dir.create("./data")}
         fileUrl <- "url of data file"
         download.file(fileUrl, destfile="./data/restaurants.csv", method="curl")
         restData <- read.csv("./data/restaurants.csv")
       
       Look at dataset
        - head(dataframe, n=3) **will show top three rows
        - tail(dataframe, n=3) **will show the bottom three rows
        - summary(dataframe)
        - str(dataframe) **more in-depth info on variables in dataframe
       
       Quantiles of quantitative variables
        - quantile(dataframe$var, na.rm=TRUE)
        - quantile(dataframe$var, probs=c(0.5, 0.75, 0.90) **lookat the quantiles
       
       Make Tables
        - table(dataframe$var, useNA="ifany") **Any missing values will have addede column
        - table(dataframe$var1, dataframe$var2)
       
       Check for Missing values
        - sum(is.na(dataframe$var)) **check number of missing
        - any(is.na(dataframe$var)) **check and see if any values = NA
        - all(dataframe$var >0) **check if all are above 0
        
       Column and row sums
        - colSum(is.na(dataframe))
        - all(colSums(is.na(dataframe))==0)
        
       Find specific characteristics
        - table(dataframe$var %in% c("value")) **find variable of a certain value
        - table(dataframe$var %in% c("value", "value")) **find multiple values of variable
        - dataframe{datatframe$var %in% c("value", "value"), ] **subset rows equal to specific values
        
       Cross tabs
        - data(dataframe)
          DF = as.data.frame(dataframe)
          summary(DF)
          xt <- xtabs(Freq ~ Gender + Admit, data=DF)
          xt
        - Flat tables
          dataframe <- rep(1:9, len = 54)
          xt <- xtabs(breaks ~., dataframe)
          xt
          ftable(xt) ** summarize data in compact form
        
        Size of dataset
          - dataframe = rnorm(le5)
            object.size(dataset)
            print(object.size(dataframe, units="Mb")
          
      ** Creating New Variables
          Raw data may not have variables you need/want
          Need to transform the data to get values
          Common variables to create
            - missingness indicators
            - cutting up quantitative variables
            - applying transformations
          
          Creating Sequences - index for your data set
            - s1 <- seq(1,10,by=2) ; s1
            - s2 <- seq(a, 10, length=3); s2
            - x <- c(1, 3, 8, 25, 100); seq(along = x)
          
          Subsetting New Variables from existing variables
            - dataframe$newvar = dataframe$var %in% c("value", "value")
              table(dataframe$newvar)
          
          Binary variables
            - dataframe$newvar = ifelse(dataframe$var < 0, TRUE, FALSE)
              table(dataframe$newgroupvar, dataframe$var > 0)
          
          Categorical out of continuous
            - dataframe$newgroupvar = cut(dataframe$var, breaks= quantile(dataframe$var))
              table(dataframe$newvar)
              table(dataframe$newgroupvar, dataframe$var)
            - Easier cutting
              library(Hmisc)
              dataframe$newgroupvar = cut2(dataframe$var, g=4)
              table(dataframe$newgroupvar)
            
           Create Factor Variables
            - dataframe$newfactorvar <- factor(dataframe$var)
              dataframe$newfactorvar[1:10]
              class(dataframe$newfactorvar)
            - Levels of factors
              newfactorvar <-= factor(var, level=c("val1", "val2"))
              relevel(factorvar, ref="yes") **set reference group
              as.numeric(newfactorvar) **change back to numeric 1 = yes, 2 = no
            
           Use Mutate function ** to create new version of variable and add to dataset
            - library(Hmisc); library(plyr)
              createnewdataframe = mutate(olddataframe, newgroupvar=cut2(var, g=1)
           
           Common Transformations
            - abs(x) absolute value
            - sqrt(x) square root
            - ceiling(x) ceiling of 3.475 = 4
            - floor(x) floor of 3.475 = 3
            - round(x, digits=n) round decimal to nearest
            - signif(x, digits=n) round to nearest 0.5
            - cos(x), sin(x)
            - log(x) natural log
            - log2(x), log10(x) other common logs
            - exp(x) exponentiate x
            
       ** Reshaping Data
            The goal is tidy data
              - Each variable form a column
              - Each observation have a row
              - Each table/file stores data on one kind of observation
            
           Reshaping Options
              - library(reshape2)
                head(dataframe)
              - Melt dataset **made dataframe tall and skinny
                dataframe$var <- rownames(dataframe)
                dataMelt <- melt(dataframe, id=c("var1name", "var2name", "var3name"), measure.vars=c("var4", "var5")) ** one row for every var 4 and var5
                head(datamelt, n=3)
                tail(datamelt, n=3)
              - Casting dataframes
                datacast <- dcast(datamelt, var1 ~ variable)
                datacast <- dcast(datamelt, var1 ~ variable, mean) **mean of values
              - Averaging Values
                head(dataframe)
                tapply(dataframe$var1, dataframe$var2, sum)
              - Split
                spIns = split(dataframe$var1, dataframe$var2)
                spIns
                newvar = lapply(spIns, sum)
                unlist(newvar)
                sapply(spIns, sum) **combine
              - plyr package
                ddply(dataframe,.(var1),summarize,sum=sum(count))
                newvar1 <- ddply(dataframe,.(var1),summarize,sum=ave(count,FUN=sum) **sum for every value of var1
              - Other functions
                acast - casting multi-dimensional arrays
                arrange - faster reordering witout order() command
                mutate - adding new variables
         
      ** Managing Dataframes with dplyr 
           dplyr package - specific for working with dataframes - made by Hadley Wickham
           Basic assumptions made by the dplyr package:
             - One observation per row
             - Each column represents a variable, measure, or characteristic
             - Use default R implementation
             - Other implementations - relational database systems
             
             dplyr commands
                - arrange: reorder rows of dataframe
                - filter: extract a subset of rows from dataframe based on logical conditions
                - select: return a subset of columns of a dataframe
                - mutate: add new variables/columns or transform existing variables
                - rename: rename variables in dataframe
                - Summarize: generate summary statistic of different variables in the dataframe
              
             dplyr Properties
                - First argument is dataframe
                - subsequent arguments describe what to do with it, and you can refer
                  to columns in the dataframe directly without using $ operator, just name
                - Result is a new dataframe
                - Dataframes must be properly formatted and annotated for this to be useful
                
             Managing Dataframes with dplyr - Basic Tools
                - library(dplyr)
                  dim(dataframe)
                  str(dataframe)
                  names(dataframe)
                - Select
                  head(select(dataframe, var1name:var2name)) **selects column between the two variables
                  head(select(dataframe, -(var1name:var2name)) **shows everything except the variables between the variables
                - Filter
                  x1 <- filter(dataframe, var1name > 30) **subset rows based on conditions
                  head(x, 10)
                  x2 <- filter(dataframe, var2name > 30 & var3name > 80) **refer to variable names directly
                - Arrange
                  dataframe <- arrange(dataframe, date) **reorder rows of dataframe based on values of columns
                  dataframe <- arrange(dataframe, desc(date))
                - Rename
                  dataframe <- rename(dataframe, newvarname = oldvarname) **rename a variable 
                - Mutate
                  dataframe <- mutate(dataframe, newvar1 = oldvar-mean(oldvar, na.rm=TRUE))**create new variable
                  dataframe <- mutate(dataframe, newvar = factor(1 * (oldvar >80), labels = c("cold", "hot")) **split dataframe by categorical variables
                  newdatastructure <- group_by(dataframe, newvar)
                  summarize(newdatastructure, newvar1 = mean(oldvar1, na.rm=TRUE), newvar2 = max(oldvar2), newvar3 = median(oldvar3))
                  dataframe <- mutate(dataframe, datevar = as.POSIXlt(datevar)$year + 1900)
                - dplyr benefits
                  Can work with other backend dataframes
                  data.table for large fast tables
                  SQL interfaces for relational databases via the DBI package
                  
             Merging Datasets
                - Merging Databases - can use merge() command
                  merge() - x, y, by, by.x, by.y, all
                  mergedData = merge(dataframe, dataframe, by.x="id variable", by.y="id", all=TRUE)
                  head(mergedData) ** default is to merge on all common column names, will create a larger dataframe if there is mismatching
                - Merging with plyr - useful for multiple dataframes with common id
                  Has to merge on thebasis of id - use join command in plyr package
                  arrange(join(dataframe1, dataframe2), id)
                  dfList = list(df1, df2, df3)
                  join_all(dfList)
                  
                  
## Week 4 ##

            Editing Text Variables
               - tolower(names(dataframe)) **makes variable all lower case
               - toupper(names(dataframe)) **makes variable all upper case
               - e.e. location.1 - splitNames = strssplit(names(dataframe), "\\.")
                 splitNames[[6]] **shows sixth element to see that variable split
               - lists 
                 mylist[[1]] **select first element
                 mylist$variable **select variable
                 firstElement <- function(x){x[1]} **remove all periods
                 sapply(splitNames, firstElement)
               - Fixing character vectors using gsub()
                 testName <- "dataset"
                 sub("_", "", testName) **replaces the first underscore
                 gsub("_", "", testName) **replaces all underscores in variable name
               - Finding values in variables
                 grep("whattofind", dataframe$variable) **will return where it appears
                 table(grep("whatofind", dataframe$variable)) **returns true and flase
                 dataframe <- dataframe[!grep("whattofind", dataframe$variable)) **make dataframe of what you find
                 grep("whattofind", dataframe$variable, value=TRUE) **shows what you ar elooking for
               - library(stringer)
                 nchar("string") **length of string
                 substrl("string", 1, 7) ** find 1-7 of string
                 str_trim("observation") **trim off excess space that appears at the end of string
             
             Important Points About Text in Data Sets
               - Names of variables should be all lower case, descriptive, not duplicated, and not underscored, have dots, or spaces
               - Variables with character values should be made into factor variables, be descriptive (TRUE/FALSE, not T/F)
               
             Regular Expressions
               - Literals (words of language) and metacharacters (grammar)
                 Consist of words that match exactly 
               - Regular expressions consits of only literals - match occurs if the sequence of literals occurs anywhere
               - Metacharacters - more general search terms
                 ^i think **match "i think" only for the start of line 
                 morning$ **match all lines with morning at the end
                 [Bb] **either a lower/upper case "b" will capture whether something is Bush or bush 
                 ^[Ii] am **look at beginning of line with upper or lower case "i" and the literal "am"
                 ^[0-9] [a-zA-Z] **specify a range of letters or numbers at the beginning of each line
                 [^?.]$ **end of line with dollar sign find ending of lines with "?" or "."
                 "." refers to any character **example 9.11 will match lines with any 9 followed by 11 (9-11, 9/11)
                 "|" flood|fire will match any lines with fire or flood in each line
                 ^([Gg]ood|[Bb]ad) **will match lines with "bad" "Bad" or "good" or "Good" at the beginning
                 ^([Gg]eroge( [Ww]\.)? ** "?" denotes optional, \ says "." is not a metacharacter, but literal dot
                 (.*) **something between parentheses and can repeated as many times ( ) or (1, 2, 3)
                 [0-9) + (.*) **"+" denotes at least one of te numbers, any possible of numbers seperated anything but numbers
                 { } are interval qualifiers, let us specify the min and max number of mataches of an expression
                 * star will match the longest possible string that satisifies the regular expression ^s(.*)s$
                 add "?" to make it so the * is not as long ^s(.*)?s$
               - Regular expressions are used in many programming languages
               - Regular expressions are literals and metacharacters that represent sets or classes of characters/words
               - Test processing via regular expressions is very powerful way to extract data from "unfriendly" sources (data not from CSVs)
               - Used with functions grep, grep1, and sub.gsub
               
            Working with Dates
               - d1 = date() **gives you time right now - class is character
               - d2 = Sys.Date() **no time, and class is Date - has different properties
                  - %d = day as number
                  - %a = abbreviated weekday
                  - %A = unabbreviated weekday
                  - %m = month (00-12)
                  - %b = abbreviated month
                  - %B = unabbreviated month
                  - %y = 2 digit year
                  - %Y = four digit year
               - Creating dates
                 x = c("1jan1960", "datestring", etc); z= as.Date(x, "%d%b%Y")
                 z[1] - z[2] **difference in time between first and second day
                 as.numeric(z[1]-[2]) **difference between dates
               - weekdays(d2) **convert to Julian
               - Lubridate package - convert number to date regardless of format
                 library(lubridate); ymd("20140108") turns into date
                 mdy(month, day, year)
                 dmy(day, month, year)
                 ymd_hms("year, month, day hour, min, secs") **turns to date object
                 ?5ys.timezone **help with timezones
                 
                 
               
            
                 
                 
                 
                 
                
           
                 
       
    
           
                
 
 
 
 
 
 
 
 
 
